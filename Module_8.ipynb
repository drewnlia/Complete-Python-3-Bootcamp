{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewnlia/Complete-Python-3-Bootcamp/blob/master/Module_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://babl.ai/wp-content/uploads/2022/02/logo_two.png\" alt=\"Drawing\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "# Introduction\n",
        "This notebook is for constructing your solutions to quiz questions in Module 8 of \"[Algorithms, AI, & Machine Learning: AI for AI Ethics.](https://courses.babl.ai/p/algorithms-ai-machine-learning)\". Make a copy in your own drive, complete any instructions below, then share it with the instructor (sheabrown@bablai.com). \n"
      ],
      "metadata": {
        "id": "BwONDaawBfhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Reinforcement Learning\n",
        "\n",
        "Reinforcement learning involves an \"agent\" that learns to take action in an environment based on the expected future reward. As we learned about in the first week when we discussed agents, figuring out the task environment is an important first step in deciding how to construct a way to make decitions. Here are a few definitions: \n",
        "\n",
        "$\\mathcal{S}$ is the set of all possible states. \\\\\n",
        "$\\mathcal{A}$ is the set of all possible actions. \\\\\n",
        "$P(s_j | s_i, a)$ are the probabilities of transitioning from state $s_i \\in \\mathcal{S}$ to state $s_j \\in \\mathcal{S}$ given that you took action $a\\in \\mathcal{A}$. \\\\\n",
        "$R(s,a)$ is a reward function that the agent gets when taking an action $a$ in state $s$. \n",
        "\n",
        "It's important to note that what counts as a state $s$ is a modeling choice, as most of the time the true state of the world (or game world) is not fully observable or known. \n",
        "\n",
        "## The Agent\n",
        "$\\pi(a | s)$ is the policy of the agent, a function that returns the action given the current state. \n",
        "$V^{\\pi}(s) = \\mathbb{E}[r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ...]$ is the value function, where $r_t \\sim R(s_t,a_t)$ is the reward at timestep $t$ and $\\gamma \\in [0,1]$ is a discount factor. The value funtion is expected discounted sum of rewards you get for starting in state $s$ and taking actions based on the policy $\\pi$ from then on. The optimal way to act in this environment is given by the optimal policy $\\pi^{*}$:\n",
        "\n",
        "\\begin{equation} \\pi^{*}(s) = \\underset{\\pi}{\\operatorname{argmax}}(V^{\\pi}(s)). \\end{equation}\n",
        "\n",
        "This is just a fancy way of saying \"find the policy that maximizes the expected future rewards from that state\". \n",
        "\n",
        "## Policy Iteration\n",
        "\n",
        "One method of model-based learning is called policy iteration. For this, we need to define the state-action value functionof a policy $Q^{\\pi}(s,a)$, which differs from the value function in that you ask what the value (discounted sum of future rewards) of being in a certain state is, but if you took action $a$ which may not have been the action of the policy in that state $\\pi(s)$. \n",
        "\n",
        "\\begin{equation} Q^{\\pi_i}(s,a) = R(s,a) + \\gamma \\sum_{s'\\in \\mathcal{S}} P(s'|s,a)V^{\\pi_i} \n",
        "\\end{equation}\n",
        "\n",
        "The reason we use the subscript $i$ is because we can search for a better policy that takes an action $a \\ne \\pi_i(s)$ that would give you more reward.\n",
        "\n",
        "\\begin{equation}\n",
        " \\pi_{i+1}(s) = \\underset{a}{\\operatorname{argmax}}(Q^{\\pi_i}(s,a))\n",
        "\\end{equation}\n",
        "\n",
        "By iteratively looping through all the states in your environment and doing this policy improvement step, you can eventually converge on the optimal policy. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oAUi8HwqfF8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Play around with OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments. The goal is to standardize how environments are defined in AI research publications so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface. \n",
        "OpenAI gym is pip-installed onto your local machine.  There are a few significant limitations to be aware of:\n",
        "\n",
        "* OpenAI Gym can not directly render animated games in Google Colab.\n",
        "\n",
        "Because OpenAI Gym requires a graphics display, the only way to display Gym in Google Colab is an embedded video.  The presentation of OpenAI Gym game animations in Google Colab is discussed later in this module.\n",
        "\n",
        "\n",
        "### Looking at Gym Environments\n",
        "\n",
        "The centerpiece of Gym is the environment, which defines the \"game\" in which your reinforcement algorithm will compete.  An environment does not need to be a game; however, it describes the following game-like features:\n",
        "* **action space**: What actions can we take on the environment, at each step/episode, to alter the environment.\n",
        "* **observation space**: What is the current state of the portion of the environment that we can observe. Usually, we can see the entire environment.\n",
        "\n",
        "Before we begin to look at Gym, it is essential to understand some of the terminology used by this library.\n",
        "\n",
        "* **Agent** - The machine learning program or model that controls the actions.\n",
        "Step - One round of issuing actions that affect the observation space.\n",
        "* **Episode** - A collection of steps that terminates when the agent fails to meet the environment's objective, or the episode reaches the maximum number of allowed steps.\n",
        "* **Render** - Gym can render one frame for display after each episode.\n",
        "* **Reward** - A positive reinforcement that can occur at the end of each episode, after the agent acts.\n",
        "* **Nondeterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space.\n",
        "\n",
        "It is important to note that many of the gym environments specify that they are not nondeterministic even though they make use of random numbers to process actions. It is generally agreed upon (based on the gym GitHub issue tracker) that nondeterministic property means that a deterministic environment will still behave randomly even when given consistent seed value. The seed method of an environment can be used by the program to seed the random number generator for the environment.\n"
      ],
      "metadata": {
        "id": "a_KDTawqpSA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #@title\n",
        "\n",
        "# Install lots of things needed to display the environments in Colab\n",
        "!wget http://www.atarimania.com/roms/Roms.rar \n",
        "!unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "!python -m atari_py.import_roms /content/ROMS >/dev/nul\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "1E66j4FX9k49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf592f20-6c66-443e-998b-bcb54b198d17"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-03 00:46:51--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19583716 (19M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar.3’\n",
            "\n",
            "Roms.rar.3          100%[===================>]  18.68M   635KB/s    in 31s     \n",
            "\n",
            "2022-12-03 00:47:22 (616 KB/s) - ‘Roms.rar.3’ saved [19583716/19583716]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (65.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gym library allows us to query some of these attributes from environments.  Below is a function to query gym environments."
      ],
      "metadata": {
        "id": "e679WbCA_3ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import gym\n",
        "\n",
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "metadata": {
        "id": "f5vqpHvT_dgZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the game\n",
        "\n",
        "Below is a function that will allow you to visualize a game session in Colab as a video. "
      ],
      "metadata": {
        "id": "KFYkL-wn-Ytu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from gym.wrappers import Monitor\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = VideoRecorder(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "aokfQ2nRiHA_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Watch a random agent play a game\n",
        "\n",
        "The code below starts a game and plays it using random actions. For this exersise, I simply want you to play around with doing something differnt that taking a random action. Look [here](https://gym.openai.com/docs/) to get some idea of how the gym environmen works. I want you to try to do three things: \n",
        "\n",
        "\n",
        "\n",
        "1.   Take any non-random action. This could be as simple as always moving to the right, or as complex as using an 'if-then' statements. to decide on an action.  \n",
        "2.   Create a memory variable of some type, and store old observations in this memory variable. Be as simple or creative as you want. \n",
        "3.  Try at least one other game (see the OpenAI website for a list).\n",
        "\n",
        "When you're done, share the document with sheabrown@bablai.com. \n",
        "\n"
      ],
      "metadata": {
        "id": "bYyI5SFRBxBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit this cell to make your changes\n",
        "# ====================================\n",
        "env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "dNtWMJPjisBK",
        "outputId": "4cfd7df4-06f2-4111-8f78-86a05fb096aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment MsPacman-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8ac5ecae4bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Edit this cell to make your changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ====================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MsPacman-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5d996598a761>\u001b[0m in \u001b[0;36mwrap_env\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./video'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Monitor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekcZsujBnIU6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}